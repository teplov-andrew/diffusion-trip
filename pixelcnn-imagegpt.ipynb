{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUp2xWkdGFu"
   },
   "source": [
    "# Homework 1: Autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0w70vkwZ-tw"
   },
   "source": [
    "## Task 1: Theory (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLGp4c5UPByO"
   },
   "source": [
    "### Problem 1: Sampling from KDE (1pt)\n",
    "\n",
    "Let $\\hat{p}_h(x) = \\frac{1}{n h^d} \\sum\\limits_{i = 1}^{n} K\\left(\\frac{x - X_i}{h}\\right)$ is the Kernel Density Estimator (see seminar 1) of a density $p_{\\pi}$, where $X_1, \\dots, X_n \\sim p_{\\pi}$, $X_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "Consider the following sampling scheme:\n",
    ">\n",
    ">1. Choose random number $k$ uniformly from the collection of numbers $\\{1, 2, \\dots, n\\}$.\n",
    ">\n",
    ">2. Sample the random variable $\\tilde{X}$ from the kernel $\\frac{1}{h^d} K\\left(\\frac{x - X_k}{h}\\right)$.\n",
    ">\n",
    "\n",
    "Prove, that $\\tilde{X}$ is distributed according to $\\hat{p}_h(x)$, i.e. the scheme above is the correct sampling scheme for $\\hat{p}_h(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0HFuytJPQ7o"
   },
   "source": [
    "$k \\sim Uniform({1,...,n})$\n",
    "\n",
    "$f_k(x) = \\frac{1}{n}$\n",
    "\n",
    "$p_{\\tilde{X}}(x) = \\sum_k P(k) p_{\\tilde{X} | k}(x).$\n",
    "\n",
    "$p_{\\tilde{X}}(x) = \\sum_{k=1} \\frac{1}{n} \\cdot \\frac{1}{h^d} K\\left(\\frac{x - X_k}{h}\\right)$\n",
    "\n",
    "$p_{\\tilde{X}}(x) =  \\frac{1}{n} \\frac{1}{h^d} \\sum_{k=1}^n  K\\left(\\frac{x - X_k}{h}\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivV4DY1SPEIL"
   },
   "source": [
    "### Problem 2: $\\alpha$-divergence (1pt)\n",
    "\n",
    "In the course, we will meet different divergences (not only $KL$). So let's get acquainted with the class of $\\alpha$-divergences:\n",
    "$$\n",
    "    D_{\\alpha}(p || q) = \\frac{4}{1 - \\alpha^2} \\left( 1 - \\int p(x)^{\\frac{1 + \\alpha}{2}}q(x)^{\\frac{1 - \\alpha}{2}}dx\\right).\n",
    "$$\n",
    "For each $\\alpha \\in [-\\infty; +\\infty]$ the function $D_{\\alpha} (p || q)$ is a measure of the similarity between two distributions. $D_{\\alpha} (p || q)$ has different properties for different $\\alpha$.\n",
    "\n",
    "Prove that for $\\alpha \\rightarrow 1$ the divergence $D_{\\alpha}(p || q) \\rightarrow KL(p || q)$, and for $\\alpha \\rightarrow -1$ the divergence $D_{\\alpha}(p || q) \\rightarrow KL(q || p)$.\n",
    "\n",
    "**Hint:** use the fact that $t^\\varepsilon = \\exp(\\varepsilon \\cdot \\ln t) = 1 + \\varepsilon \\cdot \\ln t + O(\\varepsilon^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1C_RPljZ-t3"
   },
   "source": [
    "```\n",
    "считал руками в тетради\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7wm4AOrZ-t4"
   },
   "source": [
    "### Problem 3: Curse of dimensionality (2pt)\n",
    "\n",
    "The main problem of generative modelling is the curse of dimensionality. Let try to get some intuition about it.\n",
    "\n",
    "Let consider a sphere of radius $r = 1$ in a space of $m$ dimensions. Our goal is to find the fraction of the volume of the sphere that lies between radius $r = 1 - \\epsilon$ and $r = 1$. Our geometric intuition is that this fraction is small. But the magic happens with $m$ goes to infinity.\n",
    "\n",
    "1. Find the expression of the volume of a shpere of radius $r$ in $m$ dimensions.\n",
    "\n",
    "2. Find the required fraction.\n",
    "\n",
    "3. Prove that, for large $m$, the fraction tends to 1 even for small values of $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfyWlBzOZ-t4"
   },
   "source": [
    "```\n",
    "your solution for problem 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2LH7vKPPVUt"
   },
   "source": [
    "Now it time to move on to practical part of homework.\n",
    "\n",
    "In our course we have a small util [package](https://github.com/r-isachenko/2022-DGM-Ozon-course/blob/main/homeworks/dgm_utils/utils.py) with some usefull functions for loading and visualizing the images and training plots. In each homework there is a cell with installing this package. Please read carefully what functions we have in this package. It could help you to solve the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-31T18:38:43.493449Z",
     "iopub.status.busy": "2025-01-31T18:38:43.493179Z",
     "iopub.status.idle": "2025-01-31T18:39:08.697669Z",
     "shell.execute_reply": "2025-01-31T18:39:08.696607Z",
     "shell.execute_reply.started": "2025-01-31T18:38:43.493428Z"
    },
    "id": "7wXq7VJ_SRAl",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f97a833e-b7e7-48bf-f2bc-af330516c22a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2023-DGM-MIPT-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:39:08.699158Z",
     "iopub.status.busy": "2025-01-31T18:39:08.698950Z",
     "iopub.status.idle": "2025-01-31T18:39:14.255085Z",
     "shell.execute_reply": "2025-01-31T18:39:14.254204Z",
     "shell.execute_reply.started": "2025-01-31T18:39:08.699139Z"
    },
    "id": "m-44Dxk6SRAp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, plot_training_curves\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:39:14.258483Z",
     "iopub.status.busy": "2025-01-31T18:39:14.256857Z",
     "iopub.status.idle": "2025-01-31T18:39:14.310142Z",
     "shell.execute_reply": "2025-01-31T18:39:14.309331Z",
     "shell.execute_reply.started": "2025-01-31T18:39:14.258459Z"
    },
    "id": "zk6rWePvdGFv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHouHarf_Hs-"
   },
   "source": [
    "## Task 2: PixelCNN receptive field and autocompletion on MNIST (4pt)\n",
    "\n",
    "[PixelCNN](https://arxiv.org/abs/1601.06759) model uses masked causal convoultions on images, we have discussed this model on the lecture 1 and seminar 2.\n",
    "\n",
    "The PixelCNN model is a powerful model. But the model has drawbacks.\n",
    "\n",
    "1. The model is sequential and sampling is really slow (it is a drawback of all AR models).\n",
    "\n",
    "2. The receptive field of the model is not so large. Even if the model is well-trained, the samples do not have long-range history.\n",
    "\n",
    "We will analyze these drawbacks.\n",
    "\n",
    "But first of all we need to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T18:39:14.311940Z",
     "iopub.status.busy": "2025-01-31T18:39:14.311693Z",
     "iopub.status.idle": "2025-01-31T18:39:16.339974Z",
     "shell.execute_reply": "2025-01-31T18:39:16.339173Z",
     "shell.execute_reply.started": "2025-01-31T18:39:14.311920Z"
    },
    "id": "PiOBcSvx_PN8",
    "outputId": "65a1c8fe-b19a-4c22-8f41-2fa0e8362835",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elXdTtB7_6Xl"
   },
   "source": [
    "Masked Convolution Layer is the basic building block of PixelCNN model. Look carefully at this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:59:13.427772Z",
     "iopub.status.busy": "2025-01-31T15:59:13.427467Z",
     "iopub.status.idle": "2025-01-31T15:59:13.444478Z",
     "shell.execute_reply": "2025-01-31T15:59:13.443797Z",
     "shell.execute_reply.started": "2025-01-31T15:59:13.427748Z"
    },
    "id": "ybALnsgO_PRW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.conv2d(input, self.weight * self.mask, self.bias, padding=self.padding)\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, : k // 2] = 1\n",
    "        self.mask[:, :, k // 2, : k // 2] = 1\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[:, :, k // 2, k // 2] = 1\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPw0osOrAJFm"
   },
   "source": [
    "[Layer Normalization](https://arxiv.org/abs/1607.06450) helps to stabilize training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:59:14.877880Z",
     "iopub.status.busy": "2025-01-31T15:59:14.877589Z",
     "iopub.status.idle": "2025-01-31T15:59:14.882315Z",
     "shell.execute_reply": "2025-01-31T15:59:14.881403Z",
     "shell.execute_reply.started": "2025-01-31T15:59:14.877858Z"
    },
    "id": "npUNrimpAGUZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def __init__(self, n_filters: int) -> None:\n",
    "        super().__init__(n_filters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x = super().forward(x)\n",
    "        return x.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11_PstPkAXig"
   },
   "source": [
    "Now we are ready to define the main PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:59:15.251230Z",
     "iopub.status.busy": "2025-01-31T15:59:15.250890Z",
     "iopub.status.idle": "2025-01-31T15:59:15.259775Z",
     "shell.execute_reply": "2025-01-31T15:59:15.258718Z",
     "shell.execute_reply.started": "2025-01-31T15:59:15.251173Z"
    },
    "id": "xC6E-hrHALYf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: tuple[int],\n",
    "        n_filters: int = 64,\n",
    "        kernel_size: int = 7,\n",
    "        n_layers: int = 5,\n",
    "        use_layer_norm: bool = True,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        model = [MaskedConv2d(\"A\", 1, n_filters, kernel_size=kernel_size)]\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            if use_layer_norm:\n",
    "                model.append(LayerNorm(n_filters))\n",
    "            model.append(nn.ReLU())\n",
    "            model.append(\n",
    "                MaskedConv2d(\"B\", n_filters, n_filters, kernel_size=kernel_size)\n",
    "            )\n",
    "\n",
    "        model.extend(\n",
    "            [\n",
    "                nn.ReLU(),\n",
    "                MaskedConv2d(\"B\", in_channels=n_filters, out_channels=2, kernel_size=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        out = (x.float() - 0.5) / 0.5\n",
    "        out = self.net(out)\n",
    "        return out.view(batch_size, 2, 1, *self.input_shape)\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # our loss is just cross entropy\n",
    "        total_loss = F.cross_entropy(self(x), x.long())\n",
    "        return {\"total_loss\": total_loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        samples = torch.zeros(n, 1, *self.input_shape).to(self.device)\n",
    "        for r in range(self.input_shape[0]):\n",
    "            for c in range(self.input_shape[1]):\n",
    "                logits = self(samples)[:, :, :, r, c]\n",
    "                probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                samples[:, 0, r, c] = torch.multinomial(probs, num_samples=1).squeeze(\n",
    "                    -1\n",
    "                )\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "f1ef05dd65e545c68331cd99f599d193",
      "ca20009ab92841a9bfdfbe04446dff94",
      "0ce6fe22b4724d5ebdd3f781eaa3276a",
      "c15e6d53d6f94b34a4e5d12ad95efe3f",
      "671014cd411944e7bcf25be2409025ce",
      "03f136fdd6754afca010840ea45451ee",
      "68ee25abaf3441618f344f01cd93c145",
      "7d73e1babc3b4d28954497c430214097",
      "d1cdb55c1e7a49d2b3676eaa320e8bc2",
      "c9abf59267434340a024c62afec84d14",
      "0cf88c8632e8415f9c6a80c4cd0dfc3f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T15:59:15.615180Z",
     "iopub.status.busy": "2025-01-31T15:59:15.614895Z",
     "iopub.status.idle": "2025-01-31T16:04:10.444319Z",
     "shell.execute_reply": "2025-01-31T16:04:10.443406Z",
     "shell.execute_reply.started": "2025-01-31T15:59:15.615160Z"
    },
    "id": "9FI5foNQBlr5",
    "outputId": "a89eb06c-96fb-4ec0-902a-835643aae7d9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10   \n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "N_LAYERS = 5\n",
    "N_FILTERS = 64\n",
    "USE_LAYER_NORM = True\n",
    "\n",
    "model = PixelCNN(\n",
    "    input_shape=(28, 28),\n",
    "    n_filters=N_FILTERS,\n",
    "    kernel_size=5,\n",
    "    n_layers=N_LAYERS,\n",
    "    use_layer_norm=USE_LAYER_NORM,\n",
    ")\n",
    "\n",
    "loss = model.loss(torch.zeros(1, 1, 28, 28))\n",
    "assert isinstance(loss, dict)\n",
    "assert \"total_loss\" in loss\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")\n",
    "\n",
    "test_loss = test_losses[\"total_loss\"][-1]\n",
    "print(\n",
    "    f\"Test loss: {test_loss:.2f}\",\n",
    ")\n",
    "assert test_loss < 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLR1d8HICihS"
   },
   "source": [
    "Even if the test loss is bigger than the value in assert, try to visualize train/test curves, it could find you to find the bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:14.004065Z",
     "iopub.status.busy": "2025-01-31T16:04:14.003758Z",
     "iopub.status.idle": "2025-01-31T16:04:14.180257Z",
     "shell.execute_reply": "2025-01-31T16:04:14.179390Z",
     "shell.execute_reply.started": "2025-01-31T16:04:14.004040Z"
    },
    "id": "RIiyqsSRCZXs",
    "outputId": "dc3fdcee-4a67-4a50-a191-d9b542afd49c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdMY__h8CwE5"
   },
   "source": [
    "Now we sample the new images from the model. Notice that the sampling from the autoregressive model is slow, because it is a sequential process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:14.408727Z",
     "iopub.status.busy": "2025-01-31T16:04:14.408443Z",
     "iopub.status.idle": "2025-01-31T16:04:18.394449Z",
     "shell.execute_reply": "2025-01-31T16:04:18.393698Z",
     "shell.execute_reply.started": "2025-01-31T16:04:14.408705Z"
    },
    "id": "skwg5JlRCt0s",
    "outputId": "56455041-ab08-4bcd-c851-9f2a9713330c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "samples = model.sample(25)\n",
    "show_samples(samples, title=\"MNIST samples\", nrow=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LK1ttmcqfvW"
   },
   "source": [
    "### Receptive field\n",
    "\n",
    "Let try to visualize the receptive field of the model.\n",
    "\n",
    "We should see that the receptive field grows with increasing number of convolutional layers.\n",
    "\n",
    "The receptive field can be empirically measured by backpropagating an arbitrary loss for the output features of a specific pixel with respect to the input. We implement this idea below, and visualize the receptive field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QOzj-rIOdNLs",
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:18.395940Z",
     "iopub.status.busy": "2025-01-31T16:04:18.395617Z",
     "iopub.status.idle": "2025-01-31T16:04:18.403033Z",
     "shell.execute_reply": "2025-01-31T16:04:18.402413Z",
     "shell.execute_reply.started": "2025-01-31T16:04:18.395906Z"
    },
    "id": "J1ekd7rGDBTi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_receptive_field(model: object, model_name: str) -> None:\n",
    "    input_size = (1,1,28,28)\n",
    "\n",
    "    input_tensor = torch.zeros(input_size, requires_grad=True, device=\"cuda\")\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    output = output.squeeze(2)\n",
    "\n",
    "    # print(output)\n",
    "    print(output.shape)\n",
    "\n",
    "    x_center = output.shape[2]//2\n",
    "    y_center = output.shape[3]//2\n",
    "\n",
    "    center_pixel = output[0, 0, output.shape[2]//2, output.shape[3]//2]\n",
    "    loss = center_pixel.backward()\n",
    "\n",
    "    grad = input_tensor.grad.abs().squeeze().cpu().detach().numpy()\n",
    "\n",
    "    binary_map = (grad > 1e-8).astype(np.float32)\n",
    "\n",
    "    weighted_map = (grad - grad.min()) / (grad.max() - grad.min() + 1e-8)\n",
    "\n",
    "\n",
    "    binary_map = np.stack([binary_map, binary_map, binary_map], axis=-1)\n",
    "    weighted_map = np.stack([weighted_map, weighted_map, weighted_map], axis=-1)\n",
    "\n",
    "    binary_map[x_center, y_center] = [1, 0, 0]\n",
    "    weighted_map[x_center, y_center] = [1, 0, 0]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    ax[0].imshow(weighted_map, vmin=0.0, vmax=1.0)\n",
    "    ax[1].imshow(binary_map, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax[0].set_title(f\"Weighted receptive field for {model_name}\")\n",
    "    ax[1].set_title(f\"Binary receptive field for {model_name}\")\n",
    "\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:18.404537Z",
     "iopub.status.busy": "2025-01-31T16:04:18.404310Z",
     "iopub.status.idle": "2025-01-31T16:04:19.098451Z",
     "shell.execute_reply": "2025-01-31T16:04:19.097560Z",
     "shell.execute_reply.started": "2025-01-31T16:04:18.404518Z"
    },
    "id": "ruOurWdFDIYP",
    "outputId": "1c74dd76-d582-442b-d93d-689602fcad97",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for n_layers in [1, 3, 5, 6]:\n",
    "    model = PixelCNN(\n",
    "        input_shape=(28, 28),\n",
    "        n_filters=32,\n",
    "        kernel_size=5,\n",
    "        n_layers=n_layers,\n",
    "        use_layer_norm=True,\n",
    "    )\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "    plot_receptive_field(model, model_name=f\"PixelCNN {n_layers} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Njl3VAmruh_v"
   },
   "source": [
    "You have to see that PixelCNN has strange blind spot in binary receptive field plot on the right side. This is a known issue of PixelCNN model. Please, try to understand why it happens.\n",
    "\n",
    "One way to solve this problem is a [GatedPixelCNN](https://arxiv.org/pdf/1606.05328.pdf) model (see paper, if you are interested in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8eXfFdWzoVu"
   },
   "source": [
    "### Image autocompletion\n",
    "\n",
    "One more feature of autoregressive model that we try is auto-completing an image. As autoregressive models predict pixels one by one, we can set the first pixels to predefined values and check how the model completes the image.\n",
    "\n",
    "For implementing this, we just need to skip the iterations in the sampling loop that already have a value unequals to -1.\n",
    "We redefine the sample method in our PixelCNN class to allow it to take the init of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:21.913912Z",
     "iopub.status.busy": "2025-01-31T16:04:21.913619Z",
     "iopub.status.idle": "2025-01-31T16:04:21.920394Z",
     "shell.execute_reply": "2025-01-31T16:04:21.919636Z",
     "shell.execute_reply.started": "2025-01-31T16:04:21.913888Z"
    },
    "id": "L8CBZkbPHVC2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelCNNAutoComplete(PixelCNN):\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int, init: Optional[torch.Tensor] = None) -> np.ndarray:\n",
    "        # Initialize samples tensor\n",
    "        if init is not None:\n",
    "            samples = init.clone().to(self.device).long()  \n",
    "        else:\n",
    "            samples = torch.full((n, 1, *self.input_shape), -1, device=self.device, dtype=torch.long) \n",
    "\n",
    "        for r in range(self.input_shape[0]):\n",
    "            for c in range(self.input_shape[1]):\n",
    "                mask = samples[:, 0, r, c] == -1  \n",
    "                if mask.any(): \n",
    "                    logits = self(samples)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    sampled_values = torch.multinomial(probs, num_samples=1).squeeze(-1).long() \n",
    "                    samples[:, 0, r, c][mask] = sampled_values[mask]\n",
    "\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6x2bX4Z0w5E"
   },
   "source": [
    "You have to repeat the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6a978c54158b416b838e6b4c3ec75797",
      "89ae53f699384a79a0e0ded22f070dc1",
      "2ea438daa1fc4fa6b509c97a727c355b",
      "dec26ec3485f495e8be01bc697f724cf",
      "a317cc0597d94e2d8e778639e6f4be5c",
      "bf5524848dcb488a9db8da606d832402",
      "bf1c6e25a4d947c984276071af540feb",
      "494bbf1440be461981dac3c8ec0cd94c",
      "534385ca7e3c48eba210b36c5686f62c",
      "62a76c28eaa5496a9e5d3ab4057d070c",
      "55b3f6e0a67049bfa4ebd45714481e4b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T16:04:23.836367Z",
     "iopub.status.busy": "2025-01-31T16:04:23.836022Z",
     "iopub.status.idle": "2025-01-31T16:09:17.721692Z",
     "shell.execute_reply": "2025-01-31T16:09:17.720926Z",
     "shell.execute_reply.started": "2025-01-31T16:04:23.836338Z"
    },
    "id": "qFdMAilywnI-",
    "outputId": "098a0e71-533f-41fd-d705-0e35d92b225f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = PixelCNNAutoComplete(\n",
    "    input_shape=(28, 28),\n",
    "    n_filters=N_FILTERS,\n",
    "    kernel_size=5,\n",
    "    n_layers=N_LAYERS,\n",
    "    use_layer_norm=USE_LAYER_NORM,\n",
    ")\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=10,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")\n",
    "\n",
    "assert test_losses[\"total_loss\"][-1] < 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd9rpg2I0paF"
   },
   "source": [
    "We randomly take images from the training set, mask the lower half of the image (set -1's), and let the model autocomplete it. We do this several times for each image to see the diversity of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T16:09:22.452533Z",
     "iopub.status.busy": "2025-01-31T16:09:22.452254Z",
     "iopub.status.idle": "2025-01-31T16:09:26.193677Z",
     "shell.execute_reply": "2025-01-31T16:09:26.192793Z",
     "shell.execute_reply.started": "2025-01-31T16:09:22.452511Z"
    },
    "id": "zRRlzUdyEMus",
    "outputId": "b2622894-b53a-4244-ad5f-04d55b0fb754",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def autocomplete_image(image: np.ndarray, model: object, n_samples: int) -> None:\n",
    "    image_init = image.copy()\n",
    "    image_init[:, image.shape[1] // 2 :, :] = -1\n",
    "    samples = np.stack([image, np.maximum(image_init, 0)])\n",
    "    show_samples(samples, title=\"Original image and input image to sampling\", nrow=2)\n",
    "\n",
    "    image_init = torch.tensor(image_init)\n",
    "    image_init = (\n",
    "        image_init.unsqueeze(dim=0).expand(n_samples, -1, -1, -1).to(model.device)\n",
    "    )\n",
    "    img_generated = model.sample(n_samples, image_init)\n",
    "    show_samples(img_generated, title=\"n_samples\", nrow=4)\n",
    "\n",
    "\n",
    "for i in range(1, 4):\n",
    "    autocomplete_image(train_data[i], model, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DajrWO8YJyYB"
   },
   "source": [
    "## Task 3: ImageGPT on MNIST (5pt)\n",
    "\n",
    "In this task you will try to implement the Image Transformer net for an autoregressive generation MNIST images. See the [blog](https://openai.com/blog/image-gpt/) and [paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz6to33lJydk"
   },
   "source": [
    "### Architecture\n",
    "\n",
    "Let describe the model architecture.\n",
    "\n",
    "Image Transformer consists of $L$ identical blocks similar to GPT-2 decoder blocks. These blocks are applied sequentially. The $l$th block receives a tensor $\\mathbf{h}^l$ with shape (batch_size, pixel_num, emb_dim) where pixel_num is a total number of pixels $(28*28)$ and emb_dim is a hyperparameter for the size of the embeddings.\n",
    "\n",
    "The tensor is transformed as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{n}^l &= \\text{layer_norm}(\\mathbf{h}^l), \\\\\n",
    "    \\mathbf{a}^l &= \\mathbf{h}^l + \\text{multihead_attention}(\\mathbf{n}^l), \\\\\n",
    "    \\mathbf{h}^{l+1} &= \\mathbf{a}^l + \\text{MLP}(\\text{layer_norm}(a^l)). \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have already used LayerNorm in PixelCNN, so we do not discuss it here.\n",
    "\n",
    "Each head of the multihead attention computes an embedding\n",
    "$$\n",
    "    \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}) \\mathbf{V},\n",
    "$$\n",
    "where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ - query/key/value matrices obtained using Linear projection of $\\mathbf{h}^l$.\n",
    "Then embeddings across all heads are stacked and followed by a linear layer.\n",
    "\n",
    "We will use just 2 Linear layers with ReLU activation for MLP.\n",
    "\n",
    "Embedding $h^1$ (our first embedding) is a sum of a token embedding of the input batch and learned positional embedding.\n",
    "\n",
    "After the last block we will apply a LayerNorm and a lLnear layer to obtain logits of size (batch_size, pixel_num, 2)\n",
    "$$\n",
    "\\begin{align*}\n",
    "  n^L &= \\text{layer norm}(h^L) \\\\\n",
    "  \\text{logits} &= \\text{linear}(n^L)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl_g0XJDX8Py"
   },
   "source": [
    "### Autoregressive property\n",
    "To make the model autoregressive we will introduce the following changes:\n",
    "\n",
    "1. We will apply the upper triangular mask to the matrix of attention logits ($\\mathbf{Q}\\mathbf{K}^T$). Masked values are made close to minus infinity so they will turn zero after softmax.\n",
    "2. During training we will add \"start of sequence\" token to the input tensor and pop the last pixel.\n",
    "\n",
    "Note: we will use raster order to identify which pixels come first (as we have done in PixelCNN). For each pixel the predicted probabality is conditioned on all the previous pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIZ52wPIPE2a"
   },
   "source": [
    "Let start with the multihead attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "execution": {
     "iopub.execute_input": "2025-01-31T20:21:50.802118Z",
     "iopub.status.busy": "2025-01-31T20:21:50.801757Z",
     "iopub.status.idle": "2025-01-31T20:21:50.818884Z",
     "shell.execute_reply": "2025-01-31T20:21:50.817969Z",
     "shell.execute_reply.started": "2025-01-31T20:21:50.802090Z"
    },
    "id": "lv9pX1eyQ09M",
    "outputId": "a6cf612a-af1d-470b-9bff-4b327ca22349",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.MultiheadAttention):\n",
    "    def __init__(self, embed_dim: int, num_heads: int) -> None:\n",
    "        super().__init__(embed_dim, num_heads)\n",
    "\n",
    "    def get_attention_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        attention_mask = torch.triu(torch.full((x.size(0), x.size(0)), float('-inf'), device=x.device), diagonal=1)\n",
    "\n",
    "        return attention_mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attn_mask = self.get_attention_mask(x)\n",
    "        return super().forward(x, x, x, attn_mask=attn_mask, need_weights=False)[0]\n",
    "\n",
    "\n",
    "def test_attention_mask() -> None:\n",
    "    x = torch.zeros(2, 768, 16)  # (batch_size, pixel_num, emb_dim)\n",
    "    mask = np.array([[0.0, -np.inf], [0.0, 0.0]])\n",
    "    layer = MultiheadAttention(16, 8)\n",
    "    attention_mask = layer.get_attention_mask(x)\n",
    "    assert attention_mask.size() == (x.size(0), x.size(0))\n",
    "    assert np.allclose(attention_mask.numpy(), mask)\n",
    "    out = layer(x)\n",
    "    assert x.size() == out.size()\n",
    "\n",
    "\n",
    "test_attention_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3DcBLp3T86-"
   },
   "source": [
    "Now we will define the base decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T20:21:54.337055Z",
     "iopub.status.busy": "2025-01-31T20:21:54.336725Z",
     "iopub.status.idle": "2025-01-31T20:21:54.346866Z",
     "shell.execute_reply": "2025-01-31T20:21:54.346075Z",
     "shell.execute_reply.started": "2025-01-31T20:21:54.337030Z"
    },
    "id": "yBxhyjd7h3Ww",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int) -> None:\n",
    "        \"\"\"\n",
    "        :param embed_dim: dimension of embedding space\n",
    "        :param num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.MA = MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.LN1 = nn.LayerNorm(embed_dim)\n",
    "        self.LN2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n_l = self.LN1(x)\n",
    "\n",
    "        att_output = self.MA(n_l)\n",
    "        a_l = x + att_output\n",
    "\n",
    "        n_l2 = self.LN2(a_l)\n",
    "        res = a_l + self.MLP(n_l2)\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n",
    "def test_decoder_block() -> None:\n",
    "    block = DecoderBlock(embed_dim=12, num_heads=4)\n",
    "    x = torch.zeros(4, 28, 12)\n",
    "    assert x.shape == block(x).shape\n",
    "\n",
    "\n",
    "test_decoder_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T20:21:56.126853Z",
     "iopub.status.busy": "2025-01-31T20:21:56.126525Z",
     "iopub.status.idle": "2025-01-31T20:21:56.155795Z",
     "shell.execute_reply": "2025-01-31T20:21:56.155088Z",
     "shell.execute_reply.started": "2025-01-31T20:21:56.126828Z"
    },
    "id": "2yXwW9fvVJN1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_shape: tuple[int], embed_dim: int, num_heads: int, num_layers: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_shape = input_shape\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.sos = torch.nn.Parameter(torch.zeros(embed_dim))\n",
    "        nn.init.normal_(self.sos)\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(num_embeddings=2, embedding_dim=embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_embeddings=input_shape[0] * input_shape[1]+1, embedding_dim=embed_dim)\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DecoderBlock(embed_dim=embed_dim, num_heads=num_heads))\n",
    "        self.LN_final = nn.LayerNorm(embed_dim)\n",
    "        self.linear_final = nn.Linear(embed_dim, 1, bias=False)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def add_sos_token(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = embeddings.size(1)\n",
    "\n",
    "        seq_len, batch_size, emb_dim = embeddings.shape\n",
    "\n",
    "        sos_token = self.sos.view(1, 1, emb_dim).expand(1, batch_size, emb_dim)\n",
    "        truncated = embeddings[:-1]\n",
    "\n",
    "        return torch.cat([sos_token, truncated], dim=0)\n",
    "\n",
    "    def add_pos_embeddings(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        length = embeddings.size(0)\n",
    "        position_tensor = torch.arange(length, device=self.device).unsqueeze(1)\n",
    "        embeddings += self.position_embeddings(position_tensor)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.long()\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = x.permute(1, 0)\n",
    "\n",
    "        embeddings = self.token_embeddings(x) \n",
    "        embeddings = self.add_sos_token(embeddings)\n",
    "        embeddings = self.add_pos_embeddings(embeddings)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        logits = self.linear_final(self.LN_final(embeddings))\n",
    "\n",
    "\n",
    "        return logits.permute(\n",
    "            1, 0, 2\n",
    "        ) \n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits.reshape(-1), x.reshape(-1).float())\n",
    "        return {\"total_loss\": loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples: int) -> np.ndarray:\n",
    "        # read sampling carefully\n",
    "        seq_len = self.input_shape[0] * self.input_shape[1]\n",
    "        samples = torch.zeros(n_samples, seq_len).long().to(self.device)\n",
    "        for i in range(seq_len):\n",
    "            logits = self(samples)\n",
    "            dist = torch.distributions.Bernoulli(logits=logits[:, i, 0])\n",
    "            samples[:, i] = dist.sample()\n",
    "        samples = samples.reshape(n_samples, 1, *self.input_shape)\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_image_gpt() -> None:\n",
    "    image_gpt = ImageGPT(input_shape=(2, 2), embed_dim=12, num_heads=4, num_layers=2)\n",
    "    x = torch.LongTensor([[0, 1, 0, 0], [0, 1, 1, 1]])\n",
    "    assert image_gpt(x).shape == torch.Size([2, 4, 1])\n",
    "    assert image_gpt.loss(x)[\"total_loss\"].requires_grad == True\n",
    "    assert image_gpt.sample(1).shape == torch.Size([1, 1, 2, 2])\n",
    "    img = torch.randint(2, size=(1, 1, 2, 2))\n",
    "\n",
    "\n",
    "test_image_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NptSgGtRXyca"
   },
   "source": [
    "### Model training\n",
    "\n",
    "Now we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T20:21:58.972539Z",
     "iopub.status.busy": "2025-01-31T20:21:58.972215Z",
     "iopub.status.idle": "2025-01-31T21:06:30.488152Z",
     "shell.execute_reply": "2025-01-31T21:06:30.487109Z",
     "shell.execute_reply.started": "2025-01-31T20:21:58.972514Z"
    },
    "id": "yXy7j5HoXJtE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5    \n",
    "BATCH_SIZE = 64 \n",
    "LR = 1e-3\n",
    "\n",
    "EMB_DIM = 128  \n",
    "NUM_HEADS = 8  \n",
    "NUM_LAYERS = 6 \n",
    "\n",
    "model = ImageGPT((28, 28), EMB_DIM, NUM_HEADS, NUM_LAYERS)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T21:08:44.224097Z",
     "iopub.status.busy": "2025-01-31T21:08:44.223702Z",
     "iopub.status.idle": "2025-01-31T21:08:44.431642Z",
     "shell.execute_reply": "2025-01-31T21:08:44.430652Z",
     "shell.execute_reply.started": "2025-01-31T21:08:44.224058Z"
    },
    "id": "5Udvl-csXRIL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz5t-srXk2-i"
   },
   "source": [
    "Let sample from our model. You probably get better samples than PixelCNN samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T21:08:48.663837Z",
     "iopub.status.busy": "2025-01-31T21:08:48.663483Z",
     "iopub.status.idle": "2025-01-31T21:09:25.511128Z",
     "shell.execute_reply": "2025-01-31T21:09:25.510010Z",
     "shell.execute_reply.started": "2025-01-31T21:08:48.663805Z"
    },
    "id": "jUC3JBReYCau",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "samples = model.sample(25)\n",
    "show_samples(samples, title=\"MNIST samples\", nrow=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "784c2e2ba9cdfc1ce1e8c565791a35aa78e810f3990b00899de93c9f5ea5b088"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03f136fdd6754afca010840ea45451ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ce6fe22b4724d5ebdd3f781eaa3276a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d73e1babc3b4d28954497c430214097",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d1cdb55c1e7a49d2b3676eaa320e8bc2",
      "value": 10
     }
    },
    "0cf88c8632e8415f9c6a80c4cd0dfc3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ea438daa1fc4fa6b509c97a727c355b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_494bbf1440be461981dac3c8ec0cd94c",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_534385ca7e3c48eba210b36c5686f62c",
      "value": 40
     }
    },
    "494bbf1440be461981dac3c8ec0cd94c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "534385ca7e3c48eba210b36c5686f62c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55b3f6e0a67049bfa4ebd45714481e4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62a76c28eaa5496a9e5d3ab4057d070c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "671014cd411944e7bcf25be2409025ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68ee25abaf3441618f344f01cd93c145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a978c54158b416b838e6b4c3ec75797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ae53f699384a79a0e0ded22f070dc1",
       "IPY_MODEL_2ea438daa1fc4fa6b509c97a727c355b",
       "IPY_MODEL_dec26ec3485f495e8be01bc697f724cf"
      ],
      "layout": "IPY_MODEL_a317cc0597d94e2d8e778639e6f4be5c"
     }
    },
    "7d73e1babc3b4d28954497c430214097": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ae53f699384a79a0e0ded22f070dc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf5524848dcb488a9db8da606d832402",
      "placeholder": "​",
      "style": "IPY_MODEL_bf1c6e25a4d947c984276071af540feb",
      "value": "100%"
     }
    },
    "a317cc0597d94e2d8e778639e6f4be5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf1c6e25a4d947c984276071af540feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf5524848dcb488a9db8da606d832402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c15e6d53d6f94b34a4e5d12ad95efe3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9abf59267434340a024c62afec84d14",
      "placeholder": "​",
      "style": "IPY_MODEL_0cf88c8632e8415f9c6a80c4cd0dfc3f",
      "value": " 10/10 [06:55&lt;00:00, 41.57s/it]"
     }
    },
    "c9abf59267434340a024c62afec84d14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca20009ab92841a9bfdfbe04446dff94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03f136fdd6754afca010840ea45451ee",
      "placeholder": "​",
      "style": "IPY_MODEL_68ee25abaf3441618f344f01cd93c145",
      "value": "100%"
     }
    },
    "d1cdb55c1e7a49d2b3676eaa320e8bc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dec26ec3485f495e8be01bc697f724cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62a76c28eaa5496a9e5d3ab4057d070c",
      "placeholder": "​",
      "style": "IPY_MODEL_55b3f6e0a67049bfa4ebd45714481e4b",
      "value": " 40/40 [27:42&lt;00:00, 41.62s/it]"
     }
    },
    "f1ef05dd65e545c68331cd99f599d193": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca20009ab92841a9bfdfbe04446dff94",
       "IPY_MODEL_0ce6fe22b4724d5ebdd3f781eaa3276a",
       "IPY_MODEL_c15e6d53d6f94b34a4e5d12ad95efe3f"
      ],
      "layout": "IPY_MODEL_671014cd411944e7bcf25be2409025ce"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
