{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGM0FvDvR8Gm"
   },
   "source": [
    "# Homework4: Discrete VAE + GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6rgsxFuR8Go"
   },
   "source": [
    "## Task 1: Theory (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HRlHi0jq72_"
   },
   "source": [
    "### Problem 1: ELBO surgery (1pt)\n",
    "\n",
    "In lecture 7 we proved the [ELBO surgery](http://approximateinference.org/accepted/HoffmanJohnson2016.pdf) theorem:\n",
    "$$\n",
    "    \\frac{1}{n} \\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || p(\\mathbf{z})) = KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z})) + \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}],\n",
    "$$\n",
    "where the first term is $KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z}))$ includes the aggregated posterior distribution $q_{\\text{agg}}(\\mathbf{z})$ and the prior distribution $p(\\mathbf{z})$. Our goal now is to deal with the second term. At the lecture, the second term was equal to:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\frac{1}{n}\\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || q_{\\text{agg}}(\\mathbf{z})).\n",
    "$$\n",
    "In fact, this is a mutual information between $\\mathbf{x}$ and $\\mathbf{z}$ on the empirical distribution of data and the distribution of $q(\\mathbf{z} | \\mathbf{x})$. Let treat the index of the sample $i$ as a random variable.\n",
    "$$\n",
    "    q(i, \\mathbf{z}) = q(i) q(\\mathbf{z} | i); \\quad p(i, \\mathbf{z}) = p(i) p(\\mathbf{z}); \\quad\n",
    "    q(i) = p(i) = \\frac{1}{n}.\n",
    "$$\n",
    "$$\n",
    "    \\quad q(\\mathbf{z} | i) = q(\\mathbf{z} | \\mathbf{x}_i) \\quad q_{\\text{agg}}(\\mathbf{z}) = \\sum_{i=1}^n q(i, \\mathbf{z}) = \\frac{1}{n} \\sum_{i=1}^n q(\\mathbf{z} | \\mathbf{x}_i);  \n",
    "$$\n",
    "Mutual information is a measure of independence between two random variables.\n",
    "$$\n",
    "\t\\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\mathbb{E}_{q(i, \\mathbf{z})} \\log \\frac{q(i, \\mathbf{z})}{q(i)q_{\\text{agg}}(\\mathbf{z})}.\n",
    "$$\n",
    "Prove that 2 expressions for mutual information are equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH29BjVVq_6o"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1doIiC-Mq-Dv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhBBlyW-R8Gp"
   },
   "source": [
    "### Problem 2: Gumbel-Max trick (2pt)\n",
    "\n",
    "In this problem you have to prove the Gumbel-Max trick that we have discussed on the Lecture 12.\n",
    "\n",
    "Let $\\pi_1, \\pi_2, \\dots \\pi_K, \\in (0, 1)$ and $\\sum\\limits_{k = 1}^{K} \\pi_k = 1$. Consider the discrete random variable:\n",
    "\n",
    "$$\n",
    "  c = \\arg\\max_{k} \\left[\\log \\pi_k + g_k\\right].\n",
    "$$\n",
    "\n",
    "In the formula above $g_k$ ($k \\in \\{1, \\dots K\\}$) are independent random variables distributed following the $\\text{Gumbel}(0, 1)$ distribution ([wiki](https://en.wikipedia.org/wiki/Gumbel_distribution)), i.e. $g_k \\sim \\text{Gumbel}(0, 1)$.\n",
    "\n",
    "Note that $g_k = - \\log (- \\log u)$, where $u \\sim \\text{Uniform}[0, 1]$.\n",
    "\n",
    "Our goal is to prove that $c \\sim \\text{Categorical}(\\pi_1, \\dots \\pi_K)$.\n",
    "\n",
    "1. Find cumulative distribution function ($F_{g}(x) = P(g < x)$) of Gumbel distribution.\n",
    "\n",
    "2. Find density of the Gumbel distribution (derivative of cdf).\n",
    "\n",
    "3. Consider random variables $\\zeta_k = \\log \\pi_k + g_k$. Let's fix $k^* \\in \\{1, \\dots K\\}$ and look at the following probability $P\\bigl( \\{\\zeta_{k} \\leq \\zeta_{k^*}\\} \\text{ for all } k \\neq k^*\\bigr)$. Prove that\n",
    "\n",
    "$$\n",
    "  P\\bigl( \\bigcap\\limits_{k \\neq k^*} \\{\\zeta_{k} \\leq \\zeta_{k^*}\\}\\bigr) = \\pi_{k^*}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb8f7MlqR8Gq"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnrrROPfLH7H"
   },
   "source": [
    "### Problem 3: Least Squares GAN (1pt)\n",
    "    \n",
    "The Vanilla GAN often suffers from problems with a vanishing gradient. [Least Squares GAN](https://arxiv.org/abs/1611.04076) tries to solve this problem by replacing the error function with the following:\n",
    "$$\n",
    "   \t\\min_D V(D) = \\min_D \\frac{1}{2}\\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} (D(\\mathbf{x}) - b)^2 + \\mathbb{E}_{p(\\mathbf{z})} (D(G(\\mathbf{z})) - a)^2 \\right]\n",
    "$$\n",
    "$$\n",
    "   \t\\min_G V(G) = \\min_G \\frac{1}{2}\\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} (D(\\mathbf{x}) - c)^2 + \\mathbb{E}_{p(\\mathbf{z})} (D(G(\\mathbf{z})) - c)^2 \\right],\n",
    "$$\n",
    "where $a,b,c \\in \\mathbb{R}$ some fixed constants.\n",
    "\n",
    "1) Write out the formula for the optimal discriminator $D^*$.\n",
    "  \n",
    "2) Write out the expression for the error function of the generator $V(G)$ in the case of an optimal discriminator $D^*$.\n",
    "  \n",
    "3) Prove that for $b - c = 1$, $b - a = 2$, the error function of the generator $V(G)$ in the case of the optimal discriminator $D^*$ takes the form:\n",
    "$$\n",
    "   \tV(G) = \\chi^2_{\\text{Pearson}} \\left(\\frac{\\pi(\\mathbf{x}) + p(\\mathbf{x} | \\boldsymbol{\\theta})}{2} || p(\\mathbf{x} | \\boldsymbol{\\theta})\\right),\n",
    "$$\n",
    "where $\\chi^2_{\\text{Pearson}} (p || q)$ is a squared Pearson divergence:\n",
    "$$\n",
    "   \t\\chi^2_{\\text{Pearson}} (p || q) = \\int \\frac{(p(\\mathbf{x}) - q(\\mathbf{x}))^2}{p(\\mathbf{x})} d \\mathbf{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv9kBQRBLIF6"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T16:58:33.893464Z",
     "iopub.status.busy": "2025-04-15T16:58:33.892576Z",
     "iopub.status.idle": "2025-04-15T17:00:11.864304Z",
     "shell.execute_reply": "2025-04-15T17:00:11.863261Z",
     "shell.execute_reply.started": "2025-04-15T16:58:33.893430Z"
    },
    "id": "fJQYtKgA5ate",
    "outputId": "adcf101b-3f45-4512-93ab-f2d6574d4654",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2023-DGM-MIPT-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:00:11.866331Z",
     "iopub.status.busy": "2025-04-15T17:00:11.865951Z",
     "iopub.status.idle": "2025-04-15T17:00:18.752128Z",
     "shell.execute_reply": "2025-04-15T17:00:18.751552Z",
     "shell.execute_reply.started": "2025-04-15T17:00:11.866306Z"
    },
    "id": "Sl59dviN5ekr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, show_samples, plot_training_curves\n",
    "from dgm_utils import visualize_images, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:00:18.753119Z",
     "iopub.status.busy": "2025-04-15T17:00:18.752832Z",
     "iopub.status.idle": "2025-04-15T17:00:19.338424Z",
     "shell.execute_reply": "2025-04-15T17:00:19.337815Z",
     "shell.execute_reply.started": "2025-04-15T17:00:18.753101Z"
    },
    "id": "qCNBV60b8CHX",
    "outputId": "a68a0ae1-62fc-4cf5-d4e4-70cbe26cdfc6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "print(\"cuda is available:\", USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:00:19.339901Z",
     "iopub.status.busy": "2025-04-15T17:00:19.339566Z",
     "iopub.status.idle": "2025-04-15T17:00:20.061789Z",
     "shell.execute_reply": "2025-04-15T17:00:20.061079Z",
     "shell.execute_reply.started": "2025-04-15T17:00:19.339882Z"
    },
    "id": "iGWTaTpNIwce",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# do not change this function\n",
    "def plot_losses(losses: np.ndarray, title: str):\n",
    "    n_itr = len(losses)\n",
    "    xs = np.arange(n_itr)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(xs, losses)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Iterations\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raGa3C3tR8G2"
   },
   "source": [
    "## Task 2: VQ-VAE on MNIST (5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVuKyJiZR8G2"
   },
   "source": [
    "### Training of VQ-VAE model\n",
    "\n",
    "In this part you will train [VQ-VAE](https://arxiv.org/abs/1711.00937) model that we have discussed at Lecture 12 (see also [VQ-VAE-2](https://arxiv.org/abs/1906.00446) paper).\n",
    "\n",
    "We will you MNIST dataset in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:02:25.181724Z",
     "iopub.status.busy": "2025-04-15T17:02:25.181192Z",
     "iopub.status.idle": "2025-04-15T17:02:29.425618Z",
     "shell.execute_reply": "2025-04-15T17:02:29.424852Z",
     "shell.execute_reply.started": "2025-04-15T17:02:25.181698Z"
    },
    "id": "CUgY9SH_R8G2",
    "outputId": "1b6d137c-352b-4fff-d664-e44251d4419e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQZ-eqJ2R8G2"
   },
   "source": [
    "VQ-VAE model is a VAE model with discrete latent variable.  \n",
    "\n",
    "**Reminder:**\n",
    "* We define  dictionary (word book) space $\\{\\mathbf{e}_k\\}_{k=1}^K$, where $\\mathbf{e}_k \\in \\mathbb{R}^C$, $K$ is the size of the dictionary.\n",
    "* $\\mathbf{z}_e = \\text{NN}_e(\\mathbf{x}, \\boldsymbol{\\phi})$ - continuous output of encoder network.\n",
    "* $\\mathbf{z}_q = \\mathbf{e}_{k^*}$ is a quantized representation, where $k^* = \\text{argmin}_k \\| \\mathbf{z} - \\mathbf{e}_k \\|$. It is simple nearest neighbor look up.\n",
    "* Out deterministic variational posterior:\n",
    "$$\n",
    "  q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}) = \\begin{cases}\n",
    "  1 , \\quad \\text{for } k^* = \\text{argmin}_k \\| \\mathbf{z}_e - \\mathbf{e}_k \\|; \\\\\n",
    "  0, \\quad \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "* Prior distribution is uniform: $p(c) = \\text{Uniform}\\{1, \\dots, K\\}$.\n",
    "* KL divergence between posterior and prior:\n",
    "$$\n",
    "  KL(q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}), p(c)) = \\log K.\n",
    "$$\n",
    "* ELBO:\n",
    "$$\n",
    "\t\t\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})  = \\mathbb{E}_{q(c | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{e}_{c} , \\boldsymbol{\\theta}) - \\log K =  \\log p(\\mathbf{x} | \\mathbf{z}_q, \\boldsymbol{\\theta}) - \\log K.\n",
    "$$\n",
    "* Vector quantization is non-differentiable operation. We will use **straight-through** gradient estimator (we will copy gradients from decoder input $\\mathbf{z}_q$ to encoder output $\\mathbf{z}_e$.\n",
    "\n",
    "**Important modifications:**\n",
    "Due to the straight-through gradient estimation of mapping from $\\mathbf{z}_e$ to $\\mathbf{z}_q$, the embeddings $\\mathbf{e}$ receive no gradients from the ELBO.\n",
    "\n",
    "Therefore, in order to learn the embedding space we add l2 loss (**codebook loss**) to move the embedding vectors $\\mathbf{e}$ towards the encoder outputs $\\mathbf{z}_e$.\n",
    "\n",
    "Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the embeddings $\\mathbf{e}$ do not train as fast as the encoder parameters. To make sure the encoder commits to an embedding and its output does not grow, we add a **commitment loss**.\n",
    "\n",
    "Thus, the total training objective becomes:\n",
    "$$\n",
    "  \\log p(\\mathbf{x}| \\mathbf{z}_q, \\boldsymbol{\\theta}) + \\| \\text{stop_gradient}(\\mathbf{z}_e) - \\mathbf{e}\\|_2^2 + \\| \\mathbf{z}_e - \\text{stop_gradient}(\\mathbf{e})\\|_2.\n",
    "$$\n",
    "\n",
    "Pay attention to the $\\text{stop_gradient}(*)$ operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbdLrL-_R8G2"
   },
   "source": [
    "Our first step is implement vector quantization procedure. It will also calculate two consistency losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdLprcY3kDTr"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab0x-GLckGnL"
   },
   "source": [
    "`def get_code_indices:`  \n",
    "    $$\n",
    "    \\|a - b\\|^2 = \\|a\\|^2 + \\|b\\|^2 - 2\\langle a, b \\rangle\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:02:29.817403Z",
     "iopub.status.busy": "2025-04-15T17:02:29.817129Z",
     "iopub.status.idle": "2025-04-15T17:02:29.871384Z",
     "shell.execute_reply": "2025-04-15T17:02:29.870581Z",
     "shell.execute_reply.started": "2025-04-15T17:02:29.817382Z"
    },
    "id": "jMyf05nDR8G2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 16, beta: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def get_code_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape[:-1]\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) calculate distances from flatten inputs to embeddings\n",
    "        # 2) find nearest embeddings to each input (use argmin op)\n",
    "\n",
    "        distances = (torch.sum(flattened ** 2, dim=1, keepdim=True) +\n",
    "                     torch.sum(self.embedding.weight ** 2, dim=1) -\n",
    "                     2 * torch.matmul(flattened, self.embedding.weight.t())\n",
    "                     )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "\n",
    "        # ====\n",
    "        encoding_indices = encoding_indices.view(input_shape)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_quantized(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get embeddgins with appropriate indices\n",
    "        # 2) transform tensor from BHWC to BCHW format\n",
    "        quantized = self.embedding(encoding_indices).permute(0, 3, 1, 2).contiguous()\n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get indices\n",
    "        # 2) get quantized latents\n",
    "        # 3) calculate codebook and commitment loss\n",
    "        #    do not afraid about stop_gradient op\n",
    "        #    (use .detach() method for quantized latents and x)\n",
    "        # 4) final loss is codebook_loss + beta * commitment_loss\n",
    "\n",
    "        quantized = self.get_quantized(self.get_code_indices(x))\n",
    "\n",
    "        loss = torch.mean((quantized.detach() - x)**2) + self.beta * torch.mean((quantized - x.detach())**2)\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # Straight-through estimator (think about it!).\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "def test_vector_quantizer():\n",
    "    x = torch.zeros((1, 16, 7, 7))\n",
    "    layer = VectorQuantizer()\n",
    "    indices = layer.get_code_indices(x)\n",
    "    assert indices.shape == (1, 7, 7)\n",
    "    quantized = layer.get_quantized(indices)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    quantized, loss = layer(x)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    assert loss.shape == ()\n",
    "\n",
    "\n",
    "test_vector_quantizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264s0fmGR8G2"
   },
   "source": [
    "We will use simple encoder/decoder with several strided convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:02:32.437501Z",
     "iopub.status.busy": "2025-04-15T17:02:32.437016Z",
     "iopub.status.idle": "2025-04-15T17:02:32.444896Z",
     "shell.execute_reply": "2025-04-15T17:02:32.444181Z",
     "shell.execute_reply.started": "2025-04-15T17:02:32.437477Z"
    },
    "id": "3yOFvfbDR8G2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with Conv2d and ReLU activation\n",
    "        # ====\n",
    "\n",
    "        encoder = []\n",
    "        encoder.append(nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "        encoder.append(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "        encoder.append(nn.Conv2d(64, latent_dim, kernel_size=3, stride=1, padding=1))\n",
    "        encoder.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.net = nn.Sequential(*encoder)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with ConvTransposed2d and ReLU activation\n",
    "        # ====\n",
    "\n",
    "        decoder = []\n",
    "        decoder.append(nn.ConvTranspose2d(latent_dim, 64, kernel_size=3, stride=1, padding=1))\n",
    "        decoder.append(nn.ReLU(inplace=True))\n",
    "        decoder.append(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1))\n",
    "        decoder.append(nn.ReLU(inplace=True))\n",
    "        decoder.append(nn.ConvTranspose2d(32, 2, kernel_size=4, stride=2, padding=1))\n",
    "        # decoder.append(nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(*decoder)\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92b4_3vfR8G2"
   },
   "source": [
    "Now we are ready to define our model. It consists of encoder, decoder and vector quatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:02:33.913064Z",
     "iopub.status.busy": "2025-04-15T17:02:33.912793Z",
     "iopub.status.idle": "2025-04-15T17:02:34.896255Z",
     "shell.execute_reply": "2025-04-15T17:02:34.895707Z",
     "shell.execute_reply.started": "2025-04-15T17:02:33.913043Z"
    },
    "id": "H_6RJDcxR8G2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQVAEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ce_loss_scale: float = 1.0,\n",
    "        latent_dim: int = 16,\n",
    "        num_embeddings: int = 64,\n",
    "        latent_size: tuple = (7, 7),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "        self.ce_loss_scale = ce_loss_scale\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) apply vector quantizer (it returns quantized representation + vq_loss)\n",
    "        # 3) apply decoder (it returns decoded samples)\n",
    "\n",
    "        # ====\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        q, vq_loss = self.vq_layer(z)\n",
    "\n",
    "        decoded = self.decoder(q)\n",
    "\n",
    "        return decoded, vq_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model\n",
    "        # 2) get cross entropy loss\n",
    "\n",
    "        # ====\n",
    "\n",
    "        decoded, vq_loss = self.forward(x)\n",
    "        ce_loss = F.cross_entropy(decoded, x.squeeze(1).long())\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": self.ce_loss_scale * ce_loss + vq_loss,\n",
    "            \"ce_loss\": self.ce_loss_scale * ce_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def get_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) get indices of codes using vector quantizer\n",
    "        z = self.encoder(x)\n",
    "        codebook_indices = self.vq_layer.get_code_indices(z)\n",
    "        # ====\n",
    "        return codebook_indices\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # prior distribution is uniform\n",
    "        # 1) get samples from categorical distribution\n",
    "        # 2) get quantized representations using vector quantizer\n",
    "\n",
    "        indices = torch.randint(0, self.vq_layer.num_embeddings, (n, *self.latent_size), device=\"cuda:0\")\n",
    "        quantized = self.vq_layer.get_quantized(indices)\n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def sample_from_logits(self, logits: torch.Tensor) -> np.ndarray:\n",
    "        # ====\n",
    "        # your code\n",
    "        # our model will return logits, this method applies softmax and samples from the distribution\n",
    "        # 1) apply softmax to the logits\n",
    "        # 2) sample from the distribution (e.x. you could use torch.multinomial)\n",
    "        # be careful with the sizes of the tensors (may be you need to permute/reshape dimensios)\n",
    "\n",
    "        # ====\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.permute(0, 2, 3, 1).contiguous()\n",
    "        B, H, W, C = probs.shape\n",
    "        probs_flat = probs.view(-1, C)\n",
    "        samples_flat = torch.multinomial(probs_flat, num_samples=1)\n",
    "        samples = samples_flat.view(B, H, W)\n",
    "        samples = samples.unsqueeze(1)\n",
    "\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) sample from prior distribution\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from logits\n",
    "\n",
    "            # ====\n",
    "            quantized = self.prior(n)\n",
    "            logits = self.decoder(quantized)\n",
    "            samples = self.sample_from_logits(logits)\n",
    "            return samples\n",
    "\n",
    "\n",
    "def test_vqvae_model():\n",
    "    model = VQVAEModel().cuda()\n",
    "    x = torch.zeros((2, 1, 28, 28)).cuda()\n",
    "\n",
    "    encoded = model.encoder(x)\n",
    "    size = encoded.shape[2:]\n",
    "    assert size == model.latent_size\n",
    "\n",
    "    indices = model.get_indices(x)\n",
    "    assert indices.shape == (2, 7, 7)\n",
    "\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "\n",
    "    quantized = model.prior(10)\n",
    "    assert quantized.shape == (10, 16, *model.latent_size)\n",
    "\n",
    "    decoded = model.decoder(quantized)\n",
    "    assert decoded.shape == (10, 2, 28, 28)\n",
    "\n",
    "    sampled = model.sample(10)\n",
    "    assert sampled.shape == (10, 1, 28, 28)\n",
    "\n",
    "\n",
    "test_vqvae_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4j-1Uy-R8G2"
   },
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "b434099ca48e48818bc20931edc82c2a",
      "99a93ec048c142a4b5bedf58f5674c27",
      "630b78a3fcca4b54a1db17b460b62959",
      "fc8c4a844715428ca2e0c342fb70248d",
      "1a67c997503c4032bc8f7eb4dedb3070",
      "f13abc23671f4bb79ab68497e2ce8b7e",
      "6b3ea23781dc42aaaf3070947d1a53b5",
      "894546e45c054b29b64d983cd8d746b4",
      "1339bff33bed4325ac4e2ce822ddd9b5",
      "9708143d75fa412a91d2d97d63c3ef36",
      "c493935528c345f489bbf4c7829d234b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:03:09.851932Z",
     "iopub.status.busy": "2025-04-15T17:03:09.851193Z",
     "iopub.status.idle": "2025-04-15T17:11:53.066215Z",
     "shell.execute_reply": "2025-04-15T17:11:53.065488Z",
     "shell.execute_reply.started": "2025-04-15T17:03:09.851906Z"
    },
    "id": "DIPgCoCgR8G2",
    "outputId": "666a55ab-4c76-4382-8c9f-802b4cb5a18d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = 8   # any adequate value\n",
    "EPOCHS = 20       # < 30\n",
    "LR = 1e-4           # < 1e-2\n",
    "CE_SCALE = 1     # 0.01 < x < 30.0\n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "\n",
    "model = VQVAEModel(ce_loss_scale=CE_SCALE, latent_dim=16, num_embeddings=128)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    use_cuda=USE_CUDA,\n",
    "    use_tqdm=True,\n",
    "    lr=LR,\n",
    ")\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75RLttZHR8G3"
   },
   "source": [
    "Now we is able to sample from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:05.238481Z",
     "iopub.status.busy": "2025-04-15T17:13:05.238207Z",
     "iopub.status.idle": "2025-04-15T17:13:05.463527Z",
     "shell.execute_reply": "2025-04-15T17:13:05.462806Z",
     "shell.execute_reply.started": "2025-04-15T17:13:05.238459Z"
    },
    "id": "QGeLaBCwR8G3",
    "outputId": "3f5a288b-e588-44e8-831d-c8362ad3b458",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test losses\n",
    "for key, value in test_losses.items():\n",
    "    print(\"{}: {:.4f}\".format(key, value[-1]))\n",
    "\n",
    "# Samples\n",
    "samples = model.sample(100)\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")\n",
    "\n",
    "# Reconstructions\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    decoded, _ = model(x)\n",
    "    x_recon = model.sample_from_logits(decoded)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "reconstructions = reconstructions.astype(\"float32\")\n",
    "show_samples(reconstructions, title=\"Reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFR_E8mER8G3"
   },
   "source": [
    "Probably you will get bad samples :(\n",
    "\n",
    "Do not worry, may be it is OK, we will try to fix your samples! Make sure that reconstructions are almost perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p3iNP3MR8G3"
   },
   "source": [
    "Here, we will visualize latent code indices for test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:11.402183Z",
     "iopub.status.busy": "2025-04-15T17:13:11.401617Z",
     "iopub.status.idle": "2025-04-15T17:13:11.534517Z",
     "shell.execute_reply": "2025-04-15T17:13:11.533942Z",
     "shell.execute_reply.started": "2025-04-15T17:13:11.402159Z"
    },
    "id": "mbYZlnW4R8G3",
    "outputId": "b41c5a65-9f61-4c33-927b-a82e3596354a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_images = next(iter(test_loader))[:100]\n",
    "x = test_images.cuda()\n",
    "codebook_indices = model.get_indices(x).cpu().unsqueeze(1)\n",
    "\n",
    "show_samples(test_images, \"Test images\")\n",
    "show_samples(codebook_indices, \"Test codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZiyXXO1R8G3"
   },
   "source": [
    "### Training of prior autoregressive model\n",
    "\n",
    "The samples from our VQ-VAE model is not good enough. The authors of the original VQ-VAE paper proposed to train autoregressive model in the latent space after we trained VQ-VAE model.\n",
    "\n",
    "Remember we have discussed **ELBO surgery** and **aggregrated posterior**. Let recall what do we have in VAE:\n",
    "* **Training:** we get latent variables $\\mathbf{z}$ from variational posterior $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$ for every object $\\mathbf{x}$ and then applies decoder ($p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$). It means that in average decoder is applied to the latent variables from aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "* **Inference:** We apply decoder to the latent variables from prior distribution $p(\\mathbf{z})$.\n",
    "\n",
    "It means that if our aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$ and prior $p(\\mathbf{z})$ is too far from each other, then we get inconsistency.\n",
    "\n",
    "So let train to remove this inconsistency. To be concrete, let train (autoregressive) model in the latent space that will try to predict samples from the aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "\n",
    "We will use our good friend: PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:13.857091Z",
     "iopub.status.busy": "2025-04-15T17:13:13.856807Z",
     "iopub.status.idle": "2025-04-15T17:13:13.865786Z",
     "shell.execute_reply": "2025-04-15T17:13:13.865045Z",
     "shell.execute_reply.started": "2025-04-15T17:13:13.857069Z"
    },
    "id": "2Btu_EK5R8G3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.conv2d(input, self.weight * self.mask, self.bias, padding=self.padding)\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        k = self.kernel_size[0]\n",
    "        self.mask[:, :, : k // 2] = 1\n",
    "        self.mask[:, :, k // 2, : k // 2] = 1\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[:, :, k // 2, k // 2] = 1\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:14.752041Z",
     "iopub.status.busy": "2025-04-15T17:13:14.751769Z",
     "iopub.status.idle": "2025-04-15T17:13:14.864239Z",
     "shell.execute_reply": "2025-04-15T17:13:14.863440Z",
     "shell.execute_reply.started": "2025-04-15T17:13:14.752022Z"
    },
    "id": "gxKYRKjQR8G3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int = 128,\n",
    "        input_shape: tuple = (7, 7),\n",
    "        n_filters: int = 32,\n",
    "        kernel_size: int = 5,\n",
    "        n_layers: int = 5,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the sequence of MaskedConv2d -> ReLU\n",
    "        # the last layer should be MaskedConv2d (not ReLU)\n",
    "        # Note 1: the first conv layer should be of type 'A'\n",
    "        # Note 2: final output_dim in MaskedConv2d must be 2\n",
    "        layers = []\n",
    "        layers.append(MaskedConv2d(\"A\", num_embeddings, n_filters, kernel_size=kernel_size))\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(MaskedConv2d(\"B\", n_filters, n_filters, kernel_size=kernel_size))\n",
    "\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.ReLU(),\n",
    "                MaskedConv2d(\"B\", in_channels=n_filters, out_channels=num_embeddings, kernel_size=1),\n",
    "            ]\n",
    "        )\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # read the forward method carefully\n",
    "        flattened = x.view((-1, 1))\n",
    "        encodings = torch.zeros(flattened.shape[0], self.num_embeddings).cuda()\n",
    "        encodings.scatter_(1, flattened, 1)\n",
    "        encodings = encodings.view((-1, *self.input_shape, self.num_embeddings))\n",
    "        encodings = encodings.permute((0, 3, 1, 2))\n",
    "        out = self.net(encodings)\n",
    "        out = out.view(-1, self.num_embeddings, 1, *self.input_shape)\n",
    "        return out\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "\n",
    "        # ====\n",
    "        total_loss = F.cross_entropy(self(x), x.long())\n",
    "        return {\"total_loss\": total_loss}\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        samples = torch.zeros(n, 1, *self.input_shape, dtype=torch.int64).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    samples[:, 0, r, c] = torch.multinomial(\n",
    "                        probs, num_samples=1\n",
    "                    ).squeeze(-1)\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_pixelcnn():\n",
    "    model = PixelCNN().cuda()\n",
    "    x = torch.zeros((1, 1, 7, 7), dtype=torch.int64).cuda()\n",
    "    output = model(x)\n",
    "    assert output.shape == (1, 128, 1, 7, 7)\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "    samples = model.sample(10)\n",
    "    assert samples.shape == (10, 1, 7, 7)\n",
    "\n",
    "\n",
    "test_pixelcnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FscrJfPjR8G3"
   },
   "source": [
    "Now we need to get our train and test samples. Our model will predict indices of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:18.164921Z",
     "iopub.status.busy": "2025-04-15T17:13:18.164670Z",
     "iopub.status.idle": "2025-04-15T17:13:18.170126Z",
     "shell.execute_reply": "2025-04-15T17:13:18.169540Z",
     "shell.execute_reply.started": "2025-04-15T17:13:18.164904Z"
    },
    "id": "ULi53xeeykvq",
    "outputId": "ed58b1a0-ea85-43fe-a241-46475f771558",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:19.259092Z",
     "iopub.status.busy": "2025-04-15T17:13:19.258475Z",
     "iopub.status.idle": "2025-04-15T17:13:25.568462Z",
     "shell.execute_reply": "2025-04-15T17:13:25.567705Z",
     "shell.execute_reply.started": "2025-04-15T17:13:19.259070Z"
    },
    "id": "o8YXckIjR8G3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# you have to get indices of the emdeddings from the VQ-VAE model for train and test data\n",
    "INPUT_SHAPE = (7, 7)  # input shape of your latent space\n",
    "\n",
    "\n",
    "train_indices_list = []\n",
    "test_indices_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        indices = model.get_indices(x)\n",
    "        indices = indices.unsqueeze(1)\n",
    "        train_indices_list.append(indices.cpu().numpy())\n",
    "\n",
    "train_indices = np.concatenate(train_indices_list, axis=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_loader:\n",
    "        x = x.cuda()\n",
    "        indices = model.get_indices(x)\n",
    "        indices = indices.unsqueeze(1)\n",
    "        test_indices_list.append(indices.cpu().numpy())\n",
    "\n",
    "test_indices = np.concatenate(test_indices_list, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# ====\n",
    "\n",
    "assert isinstance(train_indices, np.ndarray)\n",
    "assert isinstance(test_indices, np.ndarray)\n",
    "assert train_indices.shape == (60000, 1, *INPUT_SHAPE)\n",
    "assert test_indices.shape == (10000, 1, *INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b72ba5a6ad844eeca90ed8548c291570",
      "2988f1a8a15e41e1b47a6bd53e1f4bfc",
      "9fd3f5d4a37f4b72a1b3e94350eb6508",
      "ae1a525e62644e948a801775abce987b",
      "b9f92ba2a3de4c34bada6b2fdcca3096",
      "a45a6398bdb742e0836284681c2cdf93",
      "962f46781d9647e9b7d62409ced59aab",
      "158e081ce73841adbd52c9a83078bf71",
      "c2b4b9d773a64f78a8fa7cf6f38e0238",
      "f94923c09861473e9d984372aeb394c3",
      "442e1e82fd7c4c9c8e3b3547129cd374"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:13:35.860434Z",
     "iopub.status.busy": "2025-04-15T17:13:35.860117Z",
     "iopub.status.idle": "2025-04-15T17:23:12.014278Z",
     "shell.execute_reply": "2025-04-15T17:23:12.013697Z",
     "shell.execute_reply.started": "2025-04-15T17:13:35.860409Z"
    },
    "id": "r967dypsR8G3",
    "outputId": "3157b5b9-0b31-40c8-f541-24089349c53b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters by your own\n",
    "EPOCHS = 20     # > 5\n",
    "BATCH_SIZE = 8   # any adequate value\n",
    "LR = 1e-4           # < 1e-2\n",
    "N_LAYERS = 5     # < 10\n",
    "N_FILTERS = 64    # < 128\n",
    "# ====\n",
    "\n",
    "prior_model = PixelCNN(\n",
    "    input_shape=INPUT_SHAPE, n_filters=N_FILTERS, kernel_size=5, n_layers=N_LAYERS\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(train_indices, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_indices, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(\n",
    "    prior_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:38:51.386749Z",
     "iopub.status.busy": "2025-04-15T17:38:51.386192Z",
     "iopub.status.idle": "2025-04-15T17:38:51.709395Z",
     "shell.execute_reply": "2025-04-15T17:38:51.708709Z",
     "shell.execute_reply.started": "2025-04-15T17:38:51.386728Z"
    },
    "id": "mfgCHoi228ju",
    "outputId": "555e541c-30a9-41d5-da0a-9408c83bfac8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_CeMZVkR8G3"
   },
   "source": [
    "Now we are ready to sample from our VQ-VAE model. The difference here that we will sample our embedding indices from the PixelCNN prior model instead of the Uniform prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T17:38:54.159149Z",
     "iopub.status.busy": "2025-04-15T17:38:54.158849Z",
     "iopub.status.idle": "2025-04-15T17:38:54.519708Z",
     "shell.execute_reply": "2025-04-15T17:38:54.519030Z",
     "shell.execute_reply.started": "2025-04-15T17:38:54.159122Z"
    },
    "id": "lS1MfS8dR8G3",
    "outputId": "96d4da46-fead-406e-bc0d-e3d445ff4b81",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "indices = prior_model.sample(N_SAMPLES).squeeze(1)\n",
    "quantized = model.vq_layer.get_quantized(torch.Tensor(indices).int().cuda())\n",
    "logits = model.decoder(quantized)\n",
    "samples = model.sample_from_logits(logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onSbThJtR8G3"
   },
   "source": [
    "Here you have to get samples with good enough quality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxrSs42n1v3i"
   },
   "source": [
    "## Task 3: Vanilla GAN on CIFAR10 (4pt)\n",
    "\n",
    "In this task you will implement the standard (vanilla) GAN for CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "execution": {
     "execution_failed": "2025-04-15T13:52:39.856Z",
     "iopub.execute_input": "2025-04-15T11:41:39.887929Z",
     "iopub.status.busy": "2025-04-15T11:41:39.887631Z"
    },
    "id": "WdQfVySV1rYj",
    "outputId": "2381bdba-abc2-4ef7-c5b8-40661b986dcd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"cifar10\", flatten=False, binarize=False)\n",
    "visualize_images(train_data, \"CIFAR10 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQF9-aw6CsR4"
   },
   "source": [
    "Let revise some theoretical details.\n",
    "\n",
    "The GAN objective is\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta}} \\max_{\\boldsymbol{\\phi}} \\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} \\log D_{\\boldsymbol{\\phi}}(\\mathbf{x}) + \\mathbb{E}_{p(\\mathbf{z})} \\log (1 - D_{\\boldsymbol{\\phi}}(G_{\\boldsymbol{\\theta}}(\\mathbf{z}))) \\right]\n",
    "$$\n",
    "In practice we made iterative updates for generator and discriminator. This leads us to two separate optimization task:\n",
    "- discriminator task (with fixed $G_{\\boldsymbol{\\theta}}(\\mathbf{z})$)\n",
    "$$\n",
    "\\max_{\\boldsymbol{\\phi}} \\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} \\log D_{\\boldsymbol{\\phi}}(\\mathbf{x}) + \\mathbb{E}_{p(\\mathbf{z})} \\log (1 - D_{\\boldsymbol{\\phi}}(G_{\\boldsymbol{\\theta}}(\\mathbf{z}))) \\right]\n",
    "$$\n",
    "- generator task (with fixed $D_{\\boldsymbol{\\phi}}(\\mathbf{x})$)\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\theta}} \\left[\\mathbb{E}_{p(\\mathbf{z})} \\log (1 - D_{\\boldsymbol{\\phi}}(G_{\\boldsymbol{\\theta}}(\\mathbf{z}))) \\right]\n",
    "$$\n",
    "\n",
    "Note that (1) we have maximization task for discriminator (usually we minimize losses, so we will make it negative), we dropped the second term in generator task (it does not depend on $\\boldsymbol{\\theta}$, so it is constant).\n",
    "\n",
    "Moreover, to avoid vanishing gradients we will use **non-saturated** version of generator loss:\n",
    "$$\n",
    "\\max_{\\boldsymbol{\\theta}} \\left[\\mathbb{E}_{p(\\mathbf{z})} \\log (D_{\\boldsymbol{\\phi}}(G_{\\boldsymbol{\\theta}}(\\mathbf{z}))) \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jVi3bqGGk8t"
   },
   "source": [
    "Now we have to implement our generator and discriminator models. It will be convolutional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-04-15T11:41:50.503808Z",
     "shell.execute_reply": "2025-04-15T11:41:50.503155Z",
     "shell.execute_reply.started": "2025-04-15T11:41:50.314857Z"
    },
    "id": "2c48w_1s2HaX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, n_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.n_channels = n_channels\n",
    "        self.noise = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n",
    "\n",
    "        # implement any architecture that you want\n",
    "        # for example, it could be just sequence of transposed convs with nonlinearities\n",
    "        # put sigmoid at the end to scale the model output\n",
    "\n",
    "        self.fc = nn.Linear(z_dim, n_channels * 8 * 8)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.ConvTranspose2d(n_channels, 64, kernel_size=3, stride=1, padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.shape[0], self.n_channels, 8, 8) # [batch, channels, height, width]\n",
    "        # print(x.shape)\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def sample(self, n_samples: int) -> torch.Tensor:\n",
    "        # sample from standard normal distribution (we defined it in __init__) and apply the model\n",
    "        z = self.noise.sample((n_samples, self.z_dim)).to(self.device)\n",
    "        return self.forward(z)\n",
    "\n",
    "\n",
    "def test_generator():\n",
    "    x = torch.randn((10, 32))\n",
    "    generator = Generator(z_dim=32, n_channels=32)\n",
    "    output = generator(x)\n",
    "    assert list(output.shape) == [10, 3, 32, 32]\n",
    "\n",
    "    output = generator.sample(10)\n",
    "    assert list(output.shape) == [10, 3, 32, 32]\n",
    "\n",
    "\n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:41:50.504905Z",
     "iopub.status.busy": "2025-04-15T11:41:50.504617Z",
     "iopub.status.idle": "2025-04-15T11:41:50.520988Z",
     "shell.execute_reply": "2025-04-15T11:41:50.520273Z",
     "shell.execute_reply.started": "2025-04-15T11:41:50.504881Z"
    },
    "id": "abbzZYf42TGr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_channels=64):\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # implement any architecture that you want\n",
    "        # for example, it could be just sequence of strided convs with nonlinearities\n",
    "        # put sigmoid at the end to make it classifier-like\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(64, n_channels, kernel_size=3, stride=1, padding=1))\n",
    "        # layers.append(nn.Sigmoid())\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(n_channels * 8 * 8, 1)\n",
    "\n",
    "        self.sigma = nn.Sigmoid()\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.net(x)\n",
    "        # print(output.shape)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        # print(output.shape)\n",
    "        output = self.fc(output)\n",
    "        # print(output.shape)\n",
    "        return self.sigma(output)\n",
    "\n",
    "\n",
    "def test_discriminator():\n",
    "    x = torch.randn((10, 3, 32, 32))\n",
    "    discriminator = Discriminator()\n",
    "    output = discriminator(x)\n",
    "    # print(list(output.shape))\n",
    "    assert list(output.shape) == [10, 1]\n",
    "\n",
    "\n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSyoA5BqHlzg"
   },
   "source": [
    "Now let to implement function for training our GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:41:50.522112Z",
     "iopub.status.busy": "2025-04-15T11:41:50.521906Z",
     "iopub.status.idle": "2025-04-15T11:41:50.537282Z",
     "shell.execute_reply": "2025-04-15T11:41:50.536715Z",
     "shell.execute_reply.started": "2025-04-15T11:41:50.522091Z"
    },
    "id": "9yAX8vRk5XOK",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_gan(\n",
    "    generator: object,\n",
    "    discriminator: object,\n",
    "    train_loader: object,\n",
    "    batch_size: int,\n",
    "    n_epochs: int,\n",
    "    lr: float,\n",
    "    use_cuda: bool = False,\n",
    ") -> dict:\n",
    "\n",
    "    if use_cuda:\n",
    "        discriminator = discriminator.cuda()\n",
    "        generator = generator.cuda()\n",
    "\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "\n",
    "    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0., 0.9))\n",
    "    discr_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0., 0.9))\n",
    "\n",
    "    curr_iter = 0\n",
    "    d_loss, g_loss = torch.zeros(1), torch.zeros(1)\n",
    "    batch_loss_history = {\"discriminator_losses\": [], \"generator_losses\": []}\n",
    "    for epoch_i in tqdm(range(n_epochs)):\n",
    "\n",
    "        discriminator.train()\n",
    "        generator.train()\n",
    "        for batch_i, x in enumerate(train_loader):\n",
    "            curr_iter += 1\n",
    "            x = x.to(generator.device)\n",
    "\n",
    "            # discriminator update\n",
    "            discr_optimizer.zero_grad()\n",
    "\n",
    "            # calculate discriminator loss\n",
    "            # - log (D(x_real)) - log (1 - D(x_fake))\n",
    "\n",
    "            z = generator.noise.sample((batch_size, generator.z_dim)).to(generator.device)\n",
    "            x_fake = generator(z)\n",
    "\n",
    "            d_loss = - torch.log(discriminator(x)).mean() - torch.log(1 - discriminator(x_fake)).mean()\n",
    "\n",
    "            # ====\n",
    "            d_loss.backward()\n",
    "            discr_optimizer.step()\n",
    "\n",
    "            # generator update\n",
    "            gen_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # calculate generator loss (non-saturated version)\n",
    "            # - log (D(x_fake))\n",
    "\n",
    "            z = generator.noise.sample((batch_size, generator.z_dim)).to(generator.device)\n",
    "            x_fake_for_gen = generator(z)\n",
    "\n",
    "            g_loss = - torch.log(discriminator(x_fake_for_gen)).mean()\n",
    "\n",
    "            # ====\n",
    "            g_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            batch_loss_history[\"generator_losses\"].append(g_loss.data.cpu().numpy())\n",
    "            batch_loss_history[\"discriminator_losses\"].append(\n",
    "                d_loss.data.cpu().numpy()\n",
    "            )\n",
    "\n",
    "    return batch_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230,
     "referenced_widgets": [
      "7cf63881423440778a98766df482f2f1",
      "5d57cb787dc14437a2e08ebe7df1e6a8",
      "92c93185f5114391b2f0b6b125949240",
      "441cbca2582c40d78e0bfa7e4a04a7ac",
      "8a3831fbc6f74321b7c2acd668fdf450",
      "4774c21ca83c477ea9c1ed7f0a9d36e9",
      "d59a4490b7a149efbe9b7b648cf1c8f0",
      "ab5a45917a2b4cf78da9c2abad13af5b",
      "4fb5a007e4d544c2b0ba026cdee7fc61",
      "8f20efab32aa4e7183735713214e4854",
      "6dd4326decd349b3ba1c7cbf6b6feed6"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-04-15T13:34:21.617608Z",
     "iopub.status.busy": "2025-04-15T13:34:21.617074Z",
     "iopub.status.idle": "2025-04-15T13:51:55.594848Z",
     "shell.execute_reply": "2025-04-15T13:51:55.594023Z",
     "shell.execute_reply.started": "2025-04-15T13:34:21.617587Z"
    },
    "id": "-45af5z0IU9X",
    "outputId": "8f521b55-cf98-416b-e6a5-7dc0465a0ce6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters (you have to train the model more than 20 epochs to get good results)\n",
    "BATCH_SIZE = 8  # any adequate value\n",
    "N_CHANNELS = 64  # > 32\n",
    "N_EPOCHS = 25   # > 10\n",
    "LR = 1e-4        # < 1e-3\n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "generator = Generator(z_dim=128, n_channels=N_CHANNELS)\n",
    "discriminator = Discriminator(n_channels=N_CHANNELS)\n",
    "\n",
    "train_losses = train_gan(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    lr=LR,\n",
    "    use_cuda=USE_CUDA,\n",
    ")\n",
    "\n",
    "plot_losses(train_losses[\"discriminator_losses\"], \"Discriminator loss\")\n",
    "plot_losses(train_losses[\"generator_losses\"], \"Generator loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T13:52:14.746459Z",
     "iopub.status.busy": "2025-04-15T13:52:14.745948Z",
     "iopub.status.idle": "2025-04-15T13:52:14.974050Z",
     "shell.execute_reply": "2025-04-15T13:52:14.973231Z",
     "shell.execute_reply.started": "2025-04-15T13:52:14.746437Z"
    },
    "id": "UDK6yqHpccQP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    samples = generator.sample(100)\n",
    "    samples = samples.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "show_samples(samples, title=\"CIFAR-10 GAN-generated samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1339bff33bed4325ac4e2ce822ddd9b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "158e081ce73841adbd52c9a83078bf71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a67c997503c4032bc8f7eb4dedb3070": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2988f1a8a15e41e1b47a6bd53e1f4bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a45a6398bdb742e0836284681c2cdf93",
      "placeholder": "​",
      "style": "IPY_MODEL_962f46781d9647e9b7d62409ced59aab",
      "value": "100%"
     }
    },
    "441cbca2582c40d78e0bfa7e4a04a7ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f20efab32aa4e7183735713214e4854",
      "placeholder": "​",
      "style": "IPY_MODEL_6dd4326decd349b3ba1c7cbf6b6feed6",
      "value": " 15/15 [11:53&lt;00:00, 47.53s/it]"
     }
    },
    "442e1e82fd7c4c9c8e3b3547129cd374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4774c21ca83c477ea9c1ed7f0a9d36e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fb5a007e4d544c2b0ba026cdee7fc61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d57cb787dc14437a2e08ebe7df1e6a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4774c21ca83c477ea9c1ed7f0a9d36e9",
      "placeholder": "​",
      "style": "IPY_MODEL_d59a4490b7a149efbe9b7b648cf1c8f0",
      "value": "100%"
     }
    },
    "630b78a3fcca4b54a1db17b460b62959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_894546e45c054b29b64d983cd8d746b4",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1339bff33bed4325ac4e2ce822ddd9b5",
      "value": 15
     }
    },
    "6b3ea23781dc42aaaf3070947d1a53b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6dd4326decd349b3ba1c7cbf6b6feed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cf63881423440778a98766df482f2f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d57cb787dc14437a2e08ebe7df1e6a8",
       "IPY_MODEL_92c93185f5114391b2f0b6b125949240",
       "IPY_MODEL_441cbca2582c40d78e0bfa7e4a04a7ac"
      ],
      "layout": "IPY_MODEL_8a3831fbc6f74321b7c2acd668fdf450"
     }
    },
    "894546e45c054b29b64d983cd8d746b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a3831fbc6f74321b7c2acd668fdf450": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f20efab32aa4e7183735713214e4854": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92c93185f5114391b2f0b6b125949240": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab5a45917a2b4cf78da9c2abad13af5b",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4fb5a007e4d544c2b0ba026cdee7fc61",
      "value": 15
     }
    },
    "962f46781d9647e9b7d62409ced59aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9708143d75fa412a91d2d97d63c3ef36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99a93ec048c142a4b5bedf58f5674c27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f13abc23671f4bb79ab68497e2ce8b7e",
      "placeholder": "​",
      "style": "IPY_MODEL_6b3ea23781dc42aaaf3070947d1a53b5",
      "value": "100%"
     }
    },
    "9fd3f5d4a37f4b72a1b3e94350eb6508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_158e081ce73841adbd52c9a83078bf71",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2b4b9d773a64f78a8fa7cf6f38e0238",
      "value": 20
     }
    },
    "a45a6398bdb742e0836284681c2cdf93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab5a45917a2b4cf78da9c2abad13af5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae1a525e62644e948a801775abce987b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f94923c09861473e9d984372aeb394c3",
      "placeholder": "​",
      "style": "IPY_MODEL_442e1e82fd7c4c9c8e3b3547129cd374",
      "value": " 20/20 [10:34&lt;00:00, 31.54s/it]"
     }
    },
    "b434099ca48e48818bc20931edc82c2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99a93ec048c142a4b5bedf58f5674c27",
       "IPY_MODEL_630b78a3fcca4b54a1db17b460b62959",
       "IPY_MODEL_fc8c4a844715428ca2e0c342fb70248d"
      ],
      "layout": "IPY_MODEL_1a67c997503c4032bc8f7eb4dedb3070"
     }
    },
    "b72ba5a6ad844eeca90ed8548c291570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2988f1a8a15e41e1b47a6bd53e1f4bfc",
       "IPY_MODEL_9fd3f5d4a37f4b72a1b3e94350eb6508",
       "IPY_MODEL_ae1a525e62644e948a801775abce987b"
      ],
      "layout": "IPY_MODEL_b9f92ba2a3de4c34bada6b2fdcca3096"
     }
    },
    "b9f92ba2a3de4c34bada6b2fdcca3096": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b4b9d773a64f78a8fa7cf6f38e0238": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c493935528c345f489bbf4c7829d234b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d59a4490b7a149efbe9b7b648cf1c8f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f13abc23671f4bb79ab68497e2ce8b7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f94923c09861473e9d984372aeb394c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc8c4a844715428ca2e0c342fb70248d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9708143d75fa412a91d2d97d63c3ef36",
      "placeholder": "​",
      "style": "IPY_MODEL_c493935528c345f489bbf4c7829d234b",
      "value": " 15/15 [07:41&lt;00:00, 29.45s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
